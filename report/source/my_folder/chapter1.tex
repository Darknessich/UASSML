\chapter{Теоретические и методические основы исследования} \label{ch1}

\section{Постановка задачи} \label{ch1:task}
Рассмотрим задачу построения прогностических моделей, направленных на оценку эффективности опрыскивания сельскохозяйственных культур инсектицидами при помощи БПЛА. В рамках данного исследования предполагается, что на распыление существенное влияние оказывают следующие группы факторов: 
\begin{itemize}
\item характеристики БПЛА, включая параметры его конструкции, высоту полёта, скорость и тип форсунок; 
\item свойства целевой сельскохозяйственной культуры (тип культуры, морфология листьев и пр.); 
\item погодные условия во время обработки (скорость ветра, температура воздуха и влажность).
\end{itemize}

Целевыми переменными в рамках данной задачи выступают:
\begin{itemize}
\item $y_1 \in [0, 100] \subset \mathbb{R}$ --- процент площади покрытия растительности рабочей жидкостью (далее --- \textit{покрытие});
\item $y_2 \in \mathbb{R}_{+}$ --- средний диаметр капель распыления, выраженный в микрометрах (далее --- \textit{размер капель}).
\end{itemize}

Пусть имеется выборка наблюдений, составленная на основе агрегированных и аугментированных данных, извлечённых из открытых научных публикаций \cite{Liu2025, Wu2025}, в которых представлены экспериментальные измерения эффективности опрыскивания при различных конфигурациях БПЛА, типах культур и внешних условиях. Формально, обозначим выборку $\mathcal{D}$ размера $N$ как:

\begin{equation}
	\mathcal{D} = \left\{ \left(\mathbf{x}^{(i)}, y_1^{(i)}, y_2^{(i)}\right) \right\}_{i=1}^{N},
\end{equation}
где каждый $\mathbf{x}^{(i)} \in \mathbb{R}^d$ --- вектор признаков, описывающий конкретную реализацию комбинации параметров:

\begin{equation}
\mathbf{x}^{(i)} = \left[ x_1^{(i)}, x_2^{(i)}, \ldots, x_d^{(i)} \right],
\end{equation}
включающий как числовые переменные (скорость полёта, температура воздуха и т.п.), так и категориальные (тип форсунки, сорт культуры и т.п.). Размерность пространства признаков $d$ определяется количеством отобранных характеристик. Для упрощения формулировки будем считать, что все категориальные признаки закодированы в виде one-hot или target encoding и подлежат обработке с помощью моделей машинного обучения.

Необходимо построить две аппроксимирующие модели $f_1, f_2: \mathbb{R}^d \to \mathbb{R}$, такие что:

\begin{equation}
	\hat{y}_1 = f_1(\mathbf{x}), \quad \hat{y}_2 = f_2(\mathbf{x}),
\end{equation}
где $\hat{y}_1$ и $\hat{y}_2$ --- предсказанные моделью процент покрытия и средний диаметр капель соответственно. Требуется, чтобы модели минимизировали отклонение между предсказанными и фактическими значениями на тестовой выборке с учётом выбранной функции потерь. 

Обучающая выборка сформирована путём объединения результатов независимых  исследований, найденных в научной литературе \cite{Liu2025, Wu2025}, с последующим применением процедур синтетического расширения (data augmentation), включая стохастическое варьирование условий и моделирование шумов измерения.

Таким образом, задача направлена на решение многомерной регрессии с двумя зависимыми переменными, на входе которой --- вектор признаков, характеризующих совокупность агротехнических, технических и метеорологических условий, а на выходе --- количественные оценки параметров эффективности опрыскивания, подлежащие интерпретации в прикладных агроинженерных задачах.

В ходе работы применялись и сравнивались несколько классов моделей: метод множественной линейной регрессии (MLR), регрессия опорных векторов (SVR), ансамблевые методы --- случайный лес (RF), градиентный бустинг (GBR), XGBoost и CatBoost.

\section{Обзор используемых методов} \label{ch1:methods}
\subsection{Множественная линейная регрессия (MLR)}

\textit{Множественная линейная регрессия} (multiple linear regression, MLR) --- один из базовых методов машинного обучения и статистического анализа, предназначенный для моделирования зависимости одной количественной переменной от нескольких независимых признаков. В современном виде теория MLR была подробно изложена в работах Роналда Фишера и Джона Неймена \cite{fisher1922, rao1973}.

\subsubsection{Формализация задачи}

Предполагается, что между признаками и откликом существует линейная зависимость следующего вида:

\begin{equation}
y^{(i)} = \beta_0 + \sum_{j=1}^{d} \beta_j x_j^{(i)} + \varepsilon^{(i)}, \quad i = 1, \ldots, N,
\end{equation}
где $\beta_0 \in \mathbb{R}$ --- свободный член (смещение), $\boldsymbol{\beta} = [\beta_1, \ldots, \beta_d]^\top \in \mathbb{R}^d$ --- вектор коэффициентов линейной модели, $\varepsilon^{(i)}$ --- ошибка (шум), предполагаемая независимой и нормально распределённой случайной величиной с нулевым математическим ожиданием и постоянной дисперсией: $\varepsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)$.

Цель метода --- найти такие параметры $\beta_0, \boldsymbol{\beta}$, которые минимизируют сумму квадратов отклонений модели от наблюдаемых значений, т.е. решить задачу минимизации функции ошибки:

\begin{equation}
\min_{\beta_0, \boldsymbol{\beta}} \; \sum_{i=1}^N \left( y^{(i)} - \beta_0 - \sum_{j=1}^d \beta_j x_j^{(i)} \right)^2.
\end{equation}

Обозначим через $\mathbf{X} \in \mathbb{R}^{N \times (d+1)}$ матрицу признаков с добавленным столбцом единиц (для учёта $\beta_0$), и через $\mathbf{y} \in \mathbb{R}^N$ — вектор откликов. Тогда модель можно записать в матричной форме:

\begin{equation}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon},
\end{equation}
где $\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots, \beta_d]^\top \in \mathbb{R}^{d+1}$, и задача минимизации принимает форму:

\begin{equation}
\min_{\boldsymbol{\beta}} \; \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|^2.
\end{equation}

Решение данной задачи (при условии, что $\mathbf{X}^\top \mathbf{X}$ невырождено) задаётся аналитически формулой нормальных уравнений:

\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}.
\end{equation}

\subsubsection{Преимущества и ограничения}

Полученная модель обладает рядом свойств: несмещённость оценки, минимальная дисперсия среди всех линейных несмещённых оценок (согласно теореме Гаусса—Маркова) и возможность проведения статистических тестов на значимость коэффициентов \cite{fisher1922, rao1973}.

MLR активно применяется в задачах прогноза, интерпретируемого анализа данных и оценки влияния факторов, где линейная зависимость может считаться допустимым приближением. При этом модель чувствительна к мультиколлинеарности и выбросам, что требует предварительной проверки условий применимости, таких как полнота ранга матрицы признаков и гомоскедастичность остатков.

\subsection{Регрессия опорных векторов (SVR)}

\textit{Регрессия опорных векторов} (Support Vector Regression, SVR) представляет собой обобщение метода опорных векторов (Support Vector Machines, SVM), изначально разработанного для задач классификации, на случай задач регрессии. Теоретическая основа метода была заложена в работах Вапника и др. \cite{vapnik1995, vapnik1997svr}.

В отличие от классических методов, ориентированных на минимизацию эмпирической ошибки, метод опорных векторов стремится к оптимальному компромиссу между точностью на обучающей выборке и сложностью модели, что позволяет достичь высокой обобщающей способности, особенно в условиях ограниченных данных или высокой размерности  пространства признаков.

\subsubsection{Формализация задачи}

В SVR предполагается, что модель $f$ имеет вид:

\begin{equation}
f(\mathbf{x}) = \langle \mathbf{w}, \phi(\mathbf{x}) \rangle + b,
\end{equation}
где $\phi: \mathbb{R}^d \to \mathcal{H}$ --- отображение исходного пространства признаков в высокоразмерное (возможно бесконечномерное) гильбертово пространство признаков, $\mathbf{w} \in \mathcal{H}$ --- вектор весов, $b \in \mathbb{R}$ — смещение, а $\langle \cdot, \cdot \rangle$ --- скалярное произведение в $\mathcal{H}$.

Вводится $\varepsilon$-интенсивная зона нечувствительности, в пределах которой отклонение предсказания от истинного значения не штрафуется. Тогда задача SVR формулируется как задача минимизации функционала:

\begin{equation}
\min_{\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\xi}^*} \; \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} (\xi_i + \xi_i^*),
\end{equation}

при ограничениях:

\begin{equation}
\begin{cases}
	y^{(i)} - \langle \mathbf{w}, \phi(\mathbf{x}^{(i)}) \rangle - b \leq \varepsilon + \xi_i, \\
	\langle \mathbf{w}, \phi(\mathbf{x}^{(i)}) \rangle + b - y^{(i)} \leq \varepsilon + \xi_i^*, \\
	\xi_i, \xi_i^* \geq 0, \quad i = 1, \ldots, N,
\end{cases}
\end{equation}
где $\xi_i, \xi_i^*$ --- вспомогательные переменные, отвечающие за отклонения вне $\varepsilon$-зоны, а $C > 0$ --- гиперпараметр, контролирующий компромисс между допустимой величиной ошибок и гладкостью модели.

Переходя к двойственной задаче, получаем квадратичную задачу оптимизации:

\begin{equation}
\max_{\boldsymbol{\alpha}, \boldsymbol{\alpha}^*} -\frac{1}{2} \sum_{i,j=1}^{N} (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) K(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) + \sum_{i=1}^{N} (\alpha_i - \alpha_i^*) y^{(i)} - \varepsilon \sum_{i=1}^{N} (\alpha_i + \alpha_i^*),
\end{equation}
при ограничениях:

\begin{equation}
\sum_{i=1}^{N} (\alpha_i - \alpha_i^*) = 0, \quad 0 \leq \alpha_i, \alpha_i^* \leq C.
\end{equation}

Здесь $K(\mathbf{x}, \mathbf{x}') = \langle \phi(\mathbf{x}), \phi(\mathbf{x}') \rangle$ — положительно определённая ядерная функция (линейное, полиномиальное, сигмоидальное или радиальное базисное (Гауссовское) ядро). Решение задачи позволяет выразить модель в виде:

\begin{equation}
f(\mathbf{x}) = \sum_{i=1}^{N} (\alpha_i - \alpha_i^*) K(\mathbf{x}^{(i)}, \mathbf{x}) + b.
\end{equation}

Ненулевые множители $(\alpha_i - \alpha_i^*)$ соответствуют \textit{опорным векторам} --- наблюдениям, лежащим за пределами $\varepsilon$-зоны или на её границах. Это придаёт методу свойство разреженности и высокую эффективность в больших размерностях.

\subsubsection{Преимущества и ограничения}

Метод SVR обладает высокой устойчивостью к переобучению, особенно в условиях малых выборок или высокой коррелированности признаков, благодаря геометрической интерпретации и контролю сложности модели через норму весов $\|\mathbf{w}\|$. Однако он чувствителен к выбору параметров $C$, $\varepsilon$, а также типа и параметров ядра. Решение двойственной задачи требует квадратичного программирования, что может быть вычислительно дорого при больших объёмах данных.

\subsection{Метод случайного леса (RF)}

Метод случайного леса (Random Forest, RF) представляет собой ансамблевый алгоритм машинного обучения, предложенный Лео Брейманом в 2001 году \cite{Breiman2001}. Он объединяет концепции бэггинга (bootstrap aggregating) и случайного выбора подмножества признаков при построении каждого узла дерева, что позволяет создавать устойчивые и высокообобщающие модели для задач классификации и регрессии.

\subsubsection{Формализация метода}

Метод случайного леса строит ансамбль из $B$ независимых регрессионных деревьев $\{T_b(\mathbf{x})\}_{b=1}^B$, каждое из которых обучается на бутстрэп-подвыборке $\mathcal{D}_b \subseteq \mathcal{D}$, полученной путём случайного выбора объектов с возвращением. Затем строится решение дерева $T_b$ по следующему алгоритму: в каждом узле вместо полного множества признаков $\{1, \dots, d\}$ рассматривается случайное подмножество индексов $\mathcal{M}_b \subset \{1, \dots, d\}$ размера $m \ll d$, и сплит выбирается по признаку из $\mathcal{M}_b$, минимизирующему критерий разбиения. Итоговое предсказание в задаче регрессии задаётся усреднением по ансамблю:

\begin{equation}
f(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^B T_b(\mathbf{x}).
\end{equation}

Каждое дерево $T_b$ строится до максимальной глубины без обрезки, делая его низкосмещённым, но сильно переобученным. Усреднение по ансамблю снижает дисперсию.

Случайный характер отбора наблюдений и признаков приводит к снижению корреляции между деревьями, что является критическим фактором в обеспечении хороших обобщающих свойств ансамбля. Как показал Брейман \cite{Breiman2001}, уменьшение корреляции между базовыми моделями при сохранении их точности снижает общее отклонение ансамбля:

\begin{equation}
\text{Var}(f(\mathbf{x})) = \rho \cdot \sigma^2 + \frac{1 - \rho}{B} \cdot \sigma^2,
\end{equation}
где $\rho$ — средняя корреляция между базовыми деревьями, $\sigma^2$ — их дисперсия. При $B \to \infty$, второй член стремится к нулю, и обобщающая ошибка определяется корреляцией и индивидуальной дисперсией деревьев.

\subsubsection{Преимущества и ограничения}

Метод случайного леса обладает высокой устойчивостью к переобучению, не требует масштабирования признаков и хорошо справляется как с линейно, так и с нелинейно разделимыми зависимостями. Он эффективно работает с выборками высокой размерности и может использоваться для оценки важности признаков.

Однако модель теряет интерпретируемость по сравнению с одиночными деревьями и может быть ресурсоёмкой при большом числе деревьев и глубине.

\subsection{Градиентный бустинг (GBR)} \label{ch1:methods:gbr}

Градиентный бустинг (Gradient Boosting Regression, GBR) --- это ансамблевый метод построения прогнозирующих моделей, основанный на последовательной комбинации слабых предсказателей, как правило, решающих деревьев, с целью повышения точности регрессии. Метод был впервые формализован Джеромом Фридманом в начале 2000-х годов как обобщение алгоритма AdaBoost на произвольные дифференцируемые функции потерь \cite{friedman2001}.

Градиентный бустинг реализует стратегию жадного приближения, при которой каждая последующая модель обучается на ошибках предыдущей, минимизируя выбранную функцию потерь с использованием методов градиентного спуска в функциональном пространстве.

\subsubsection{Формализация задачи}

Цель метода --- найти функцию $F(\mathbf{x})$, аппроксимирующую зависимость между признаками и откликом путём минимизации эмпирического риска:

\begin{equation}
F^*(\mathbf{x}) = \arg\min_{F \in \mathcal{F}} \sum_{i=1}^{N} \mathcal{L}(y^{(i)}, F(\mathbf{x}^{(i)})),
\end{equation}
где $\mathcal{L}$ — дифференцируемая по второму аргументу функция потерь, а $\mathcal{F}$ — класс допустимых функций (например, пространство деревьев решений).

Идея градиентного бустинга заключается в поэтапном приближении решения через функциональный градиентный спуск. На этапе $m$ при известной текущей оценке $F_{m-1}(\mathbf{x})$ вычисляются «псевдо-остатки» --- отрицательные градиенты по предсказаниям:

\begin{equation}
r_i^{(m)} = -\left.\frac{\partial L\bigl(y^{(i)},F(\mathbf{x}^{(i)})\bigr)}{\partial F(\mathbf{x}^{(i)})}\right|_{F=F_{m-1}}.
\end{equation}

Затем обучается базовый регрессор $h_m(\mathbf{x})$ на данных $\{(\mathbf{x}^{(i)},r_i^{(m)})\}$, то есть решается:

\begin{equation}
h_m = \arg\min_{h\in\mathcal{H}} \sum_{i=1}^N \bigl(r_i^{(m)} - h(\mathbf{x}^{(i)})\bigr)^2.
\end{equation}

Далее находится оптимальный шаг по линии:

\begin{equation}
\gamma_m = \arg\min_{\gamma} \sum_{i=1}^N L\bigl(y^{(i)},\,F_{m-1}(\mathbf{x}^{(i)}) + \gamma\,h_m(\mathbf{x}^{(i)})\bigr),
\end{equation}
и модель обновляется по правилу:

\begin{equation}
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu\,\gamma_m\,h_m(\mathbf{x}),
\end{equation}
где $\nu \in (0,1]$ — параметр скорости обучения (\textit{learning rate}), снижающий размер шага для повышения устойчивости.

\subsubsection{Преимущества и ограничения}

Градиентный бустинг сочетает в себе сильные стороны ансамблевых методов и градиентного спуска, обеспечивая высокую предсказательную способность даже при сложных нелинейных зависимостях и смешанных типах признаков. Благодаря параметру  $\nu$ и ограничению глубины деревьев можно контролировать сложность модели и снижать риск переобучения, а последовательная жадная схема обеспечивает интерпретируемость вклада каждого добавленного дерева \cite{hastie2009elements}. Вместе с тем метод чувствителен к настройке гиперпараметров: число итераций, скорость обучения, максимальная глубина базовых деревьев и критерий остановки должны подбираться тщательно, зачастую с помощью перекрёстной проверки. Кроме того, из-за итеративного обучения ансамбля из $M$ компонентов вычислительная и пространственная сложности метода растут линейно с $M$, что может стать ограничением при больших объёмах данных или при необходимости быстрой онлайн-обучаемости \cite{hastie2009elements}.

\subsection{XGBoost}

Метод XGBoost (Extreme Gradient Boosting) --- одна из наиболее производительных реализаций градиентного бустинга (см. п. \ref{ch1:methods:gbr}), разработанную Тяньци Ченом и Карлосом Гуэстрином \cite{Chen2016}. Он был предложен как масштабируемое и регуляризованное обобщение градиентного бустинга, способное эффективно работать с большими разреженными наборами данных и обеспечивать высокое качество модели при относительно низких затратах вычислительных ресурсов.

\subsubsection{Формализация метода}

Модель XGBoost строится в виде суммы $K$ аддитивных базовых моделей $f_k \in \mathcal{F}$, где $\mathcal{F}$ — пространство деревьев решений:

\begin{equation}
\hat{y}^{(i)} = \sum_{k=1}^{K} f_k(\mathbf{x}^{(i)}), \quad f_k \in \mathcal{F}.
\end{equation}

Каждое дерево $f_k$ представляет собой структуру, сопоставляющую входному признаковому вектору значение на одном из листьев дерева. Обучение осуществляется путём последовательного добавления новых деревьев, уменьшающих значение заданной функции потерь.

Функция оптимизации имеет вид:

\begin{equation}
\mathcal{L}^{(t)} = \sum_{i=1}^N l\left(y^{(i)}, \hat{y}^{(i)(t-1)} + f_t(\mathbf{x}^{(i)})\right) + \Omega(f_t),
\end{equation}
где $l$ — дифференцируемая функция потерь (например, среднеквадратичная ошибка), $\Omega(f)$ — регуляризационный член, накладывающий штраф за сложность модели:

\begin{equation}
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2,
\end{equation}
где $T$ — число листьев в дереве, $w_j$ — вес $j$-го листа, $\gamma$ и $\lambda$ — гиперпараметры регуляризации. Такой подход позволяет контролировать переобучение за счёт структурной регуляризации.

Для эффективной оптимизации функции потерь применяется второй порядок разложения Тейлора:

\begin{equation}
\mathcal{L}^{(t)} \approx \sum_{i=1}^N \left[ l(y^{(i)}, \hat{y}^{(i)(t-1)}) + g_i f_t(\mathbf{x}^{(i)}) + \frac{1}{2} h_i f_t^2(\mathbf{x}^{(i)}) \right] + \Omega(f_t),
\end{equation}
где $g_i = \partial_{\hat{y}^{(i)(t-1)}} l(y^{(i)}, \hat{y}^{(i)(t-1)})$ и $h_i = \partial^2_{\hat{y}^{(i)(t-1)}} l(y^{(i)}, \hat{y}^{(i)(t-1)})$ — первая и вторая производные функции потерь.

Суммарный прирост качества от добавления нового дерева рассчитывается по формуле:

\begin{equation}
\mathcal{L}_{\text{split}} = \frac{1}{2} \left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right] - \gamma,
\end{equation}
где $G_L, H_L$ и $G_R, H_R$ — суммы градиентов и гессианов для левого и правого поддеревьев соответственно. Выбор лучшего разделения узла осуществляется по максимальному приросту функции.

\subsubsection{Преимущества и ограничения}

Ключевой особенностью XGBoost является наличие регуляризации, которая способствует снижению вероятности переобучения и улучшает обобщающую способность модели. Благодаря блочной структуре представления данных, параллельных вычислений при построении деревьев и буферизации промежуточных градиентов и гессианов, алгоритм демонстрирует высокую вычислительную эффективность, особенно при работе с большими объёмами данных.

Однако, метод чувствителен к выбору гиперпараметров, от которых существенно зависит точность и устойчивость модели. При избыточной глубине деревьев или слишком большом числе итераций возможно переобучение, особенно при наличии шумов в данных. Кроме того, несмотря на наличие различных оптимизаций, при работе с действительно большими и разреженными наборами данных объём вычислений и требования к памяти могут быть значительными, что накладывает ограничения на применимость в условиях ограниченных ресурсов.

\subsection{CatBoost}

CatBoost (Categorical Boosting) --- это модификация градиентного бустинга на решающих деревьях, специально ориентированная на эффективную обработку категориальных признаков и устойчивость к переобучению. Метод разработан исследовательской группой компании Яндекс под руководством Прохоренко Л. и Гусева А. \cite{prokhorenkova2018catboost}.  CatBoost отличается от других реализаций бустинга  использованием упорядоченного бустинга, схематически предотвращающего утечку информации, и особой техникой кодирования категориальных переменных на основе таргет-статистик с контролем смещения.

\subsubsection{Формализация метода}

CatBoost вводит ключевую модификацию стандартного бустинга --- Ordered Boosting, направленную на устранение предвзятой оценки градиента, возникающей при использовании текущих предсказаний модели для оценки градиентов на тех же данных.

Вместо использования всей обучающей выборки для расчёта градиентов, CatBoost разбивает данные на набор перестановок $\sigma \in \mathfrak{S}_n$ и для каждого объекта $x_i$ использует лишь предсказания, полученные на подвыборке из объектов, предшествующих ему в перестановке $\sigma$:

\begin{equation}
\hat{g}_i^{(m)} = \left. \frac{\partial L(y_i, \hat{F}_{m-1}(x_i))}{\partial \hat{F}} \right|_{\hat{F} = \sum_{j \in \{j: \sigma(j) < \sigma(i)\}} f_j(x_j)}
\end{equation}

CatBoost использует оригинальный метод преобразования категориальных признаков, основанный на таргет-статистике с порядком. В отличие от обычного one-hot-encoding, CatBoost рассчитывает условное ожидание целевой переменной по значению категориального признака с использованием техники <<leave-one-out>> в упорядоченной последовательности:

\begin{equation}
\text{Stat}(x_i^j) = \frac{\sum_{k: \sigma(k) < \sigma(i)} \mathbf{1}[x_k^j = x_i^j] \cdot y_k + a \cdot \mu}{\sum_{k: \sigma(k) < \sigma(i)} \mathbf{1}[x_k^j = x_i^j] + a}
\end{equation}
где $a$ — параметр сглаживания, $\mu$ — глобальное среднее значения целевой переменной.

\subsubsection{Преимущества и ограничения}

Преимущества CatBoost заключаются в его способности эффективно обрабатывать категориальные признаки без необходимости ручного кодирования. Алгоритм устраняет смещение в оценке градиентов, что снижает переобучение и улучшает обобщающую способность модели, особенно на небольших и разреженных выборках.

Ограничения CatBoost связаны в первую очередь с его вычислительной сложностью: по сравнению с другими реализациями бустинга он может требовать больше времени на обучение, особенно на очень больших наборах данных.


